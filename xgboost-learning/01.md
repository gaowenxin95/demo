
# Xgboost

## 定义
![](https://img2018.cnblogs.com/blog/1365906/202002/1365906-20200223090350989-706397032.png)
[图片来源知乎](https://zhuanlan.zhihu.com/p/29765582)
GBDT的含义就是用Gradient Boosting的策略训练出来的DT模型。模型的结果是一组回归分类树组合(CART Tree Ensemble)：$T_1...T_K$ 。其中 $T_j$ 学习的是之前 $j-1$棵树预测结果的残差，这种思想就像准备考试前的复习，先做一遍习题册，然后把做错的题目挑出来，在做一次，然后把做错的题目挑出来在做一次，经过反复多轮训练，取得最好的成绩。[知乎](https://zhuanlan.zhihu.com/p/30339807)

目前我的理解就是：先随机抽取一些样本进行训练，得到一个基分类器，然后再次训练拟合模型的残差。
残差的定义：$y_{真实}-y_{预测}$，前一个基分类器未能拟合的部分也就是残差，于是新分类器继续拟合，直到残差达到指定的阈值。

## 基于残差的gradient
gradient是梯度的意思，也可以说是一阶导数
**平方损失函数MSE：**$\frac{1}{2} \sum_{0}^{n}\left(y_{i}-F\left(x_{i}\right)\right)^{2}$
熟悉其他算法的原理应该知道，这个损失函数主要针对回归类型的问题，分类则是用熵值类的损失函数。具体到平方损失函数的式子，你可能已经发现它的一阶导其实就是残差的形式，所以基于残差的GBDT是一种特殊的GBDT模型，它的损失函数是平方损失函数，常用来处理回归类的问题。具体形式可以如下表示：
**损失函数：**$L(y, F(x))=\frac{1}{2}(y-F(X))^{2}$
因此求最小化的$J=\frac{1}{2}(y-F(X))^{2}$
哈哈此使可以求一阶导数了
**损失函数的一阶导数（梯度）：**$\frac{\partial J}{\partial F\left(x_{i}\right)}=\frac{\partial \sum_{i} L\left(y_{i}, F\left(x_{i}\right)\right)}{\partial F\left(x_{i}\right)}=\frac{\partial L\left(y_{i}, F\left(x_{i}\right)\right)}{\partial F\left(x_{i}\right)}=F\left(x_{i}\right)-y_{i}$
而参数就是负的梯度：$y_{i}-F\left(x_{i}\right)=-\frac{\partial J}{\partial F\left(x_{i}\right)}$

### 评价
基于残差的GBDT在解决回归问题上不算是一个好的选择，一个比较明显的缺点就是对异常值过于敏感。
当存在一个异常值的时候，就会导致残差灰常之大。。自行理解

## boosting


gbdt模型可以认为是是由k个基模型组成的一个加法运算式

$\hat{y}_{i}=\sum_{k=1}^{K} f_{k}\left(x_{i}\right), f_{k} \in F$

其中F是指所有基模型组成的函数空间
那么一般化的损失函数是预测值 $\hat{y}_{i}$ 与 真实值$y_{i}$ 之间的关系，如我们前面的平方损失函数，那么对于n个样本来说，则可以写成
$L=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)$

更一般的，我们知道一个好的模型，在偏差和方差上有一个较好的平衡，而算法的损失函数正是代表了模型的偏差面，最小化损失函数，就相当于最小化模型的偏差，但同时我们也需要兼顾模型的方差，所以目标函数还包括抑制模型复杂度的正则项，因此目标函数可以写成
$O b j=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}\right)+\sum_{k=1}^{K} \Omega\left(f_{k}\right)$
其中 $\Omega$ 代表了基模型的复杂度，若基模型是树模型，则树的深度、叶子节点数等指标可以反应树的复杂程度。

### 公式
对于Boosting来说，它采用的是前向优化算法，即从前往后，逐渐建立基模型来优化逼近目标函数，具体过程如下：
$\hat{y}_{i}^{0}=0$
$\hat{y}_{i}^{1}=f_{1}\left(x_{i}\right)=\hat{y}_{i}^{0}+f_{1}\left(x_{i}\right)$
$\hat{y}_{i}^{2}=f_{1}\left(x_{i}\right)+f_{2}\left(x_{i}\right)=\hat{y}_{i}^{1}+f_{2}\left(x_{i}\right)$
$\cdots$
$\hat{y}_{i}^{t}=\sum_{k=1}^{t} f_{k}\left(x_{i}\right)=\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)$

### 如何学习一个新模型
关键还是在于GBDT的目标函数上，即新模型的加入总是以优化目标函数为目的的。

以第t步的模型拟合为例，在这一步，模型对第 $i$个样本 $x_i$ 的预测为：
$\hat{y}_{i}^{t}=\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)$

其中 $f_{t}\left(x_{i}\right)$ 就是我们这次需要加入的新模型，即需要拟合的模型，此时，目标函数就可以写成：

$\begin{aligned} O b j^{(t)} &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t}\right)+\sum_{i=i}^{t} \Omega\left(f_{i}\right) \\ &=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)+\Omega\left(f_{t}\right)+\text { constant } \end{aligned}$    (1)
因此当求出最优目标函数的时候也就相当于求出了$f_{t}\left(x_{i}\right)$ 

## GBDT的目标函数
**这部分也是推导XGBoost的过程**

我们知道泰勒公式中，若$\Delta x$ 很小时，我们只保留二阶导是合理的（GBDT是一阶导，XGBoost是二阶导，我们以二阶导为例，一阶导可以自己去推，因为更简单）**或许也可以说我们更希望将优化问题转化为一个凸优化问题，因此而引入二阶泰特展开式**，即：
$f(x+\Delta x) \approx f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}$   (2)

那么在等式（1）中，我们把 $\hat{y}_{i}^{t-1}$ 看成是等式（2）中的x， $f_{t}\left(x_{i}\right)$  看成是 $\Delta x$ ，因此等式（1）可以写成：

$O b j^{(t)}=\sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}_{i}^{t-1}\right)+g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)+$ constant  (3)

其中 $g_{i}$ 为损失函数的一阶导， $h_i$ 为损失函数的二阶导，注意这里的导是对 $\hat{y}_{i}^{t-1}$ 求导。我们以 平方损失函数为例$\sum_{i=1}^{n}\left(y_{i}-\left(\hat{y}_{i}^{t-1}+f_{t}\left(x_{i}\right)\right)\right)^{2}$ ，则分别给出$g_i$,$h_i$

$g_{i}=\partial_{\hat{y}^{t-1}}\left(\hat{y}^{t-1}-y_{i}\right)^{2}=2\left(\hat{y}^{t-1}-y_{i}\right), 
\quad h_{i}=\partial_{\hat{y}^{t-1}}^{2}\left(\hat{y}^{t-1}-y_{i}\right)^{2}=2$

由于在第t步 $\hat{y}_{i}^{t-1}$ 其实是一个已知的值，所以 $l\left(y_{i}, \hat{y}_{i}^{t-1}\right)$ 是一个常数，其对函数优化不会产生影响，因此，等式（3）可以写成：
$O b j^{(t)} \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)$    (4)



所以我么只要求出每一步损失函数的一阶和二阶导的值（由于前一步的 $\hat{y}_{i}^{t-1}$ 是已知的，所以这两个值就是常数）代入等式4，然后最优化目标函数，就可以得到每一步的 $f(x)$ ，最后根据加法模型得到一个整体模型

## 如何使用决策树表示目标函数

假设我们boosting的基模型用决策树来实现，则一颗生成好的决策树，即结构确定，也就是说树的叶子结点其实是确定了的。假设这棵树的叶子结点有 $T$ 片叶子，而每片叶子对应的值 $w \in R^{T}$ 。熟悉决策树的同学应该清楚，每一片叶子结点中样本的预测值都会是一样的，在**分类问题中是某一类**，**在回归问题中，是某一个值**（在GBDT中都是回归树，即分类问题转化成对概率的回归了），那么肯定存在这样一个函数$q:R^d->{1,2,...T}$,即将 $f_{t}(x)$ 中的每个样本映射到每一个叶子结点上，当然 $f_{t}(x)$和 q 我们都是不知道的，但我们也不关心，这里只是说明一下决策树表达数据结构的方法是怎么样的，不理解也没有问题。

下面来正式推导：

$f_{t}(x)$可以转化为$w_{q(x)}$,其中$q(x)$ 代表了每个样本在哪个叶子结点上,而 $w_q$ 则代表了哪个叶子结点取什么 $w$ 值，所以 $w_{q(x)}$ 就代表了每个样本的取值$w$ （即预测值.

如果决策树的复杂度可以由正则项来定义 $\Omega\left(f_{t}\right)=\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2}$ ，即决策树模型的复杂度由生成的树的叶子节点数量和叶子节点对应的值向量的L2范数决定。

我们假设 $I_{j}=\left\{i | q\left(x_{i}\right)=j\right\}$ 为第 j 个叶子节点的样本集合，则等式4根据上面的一些变换可以写成：

$\begin{aligned} O b j^{(t)} & \approx \sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right) \\ &=\sum_{i=1}^{n}\left[g_{i} w_{q\left(x_{i}\right)}+\frac{1}{2} h_{i} w_{q\left(x_{i}\right)}^{2}\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\ &=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T \end{aligned}$                                           (5)

即我们之前样本的集合，现在都改写成叶子结点的集合，由于一个叶子结点有多个样本存在，因此
![](https://img2018.cnblogs.com/blog/1365906/202002/1365906-20200223132914561-1321075209.png)

## 如何优化目标函数
那么对于单棵决策树，一种理想的优化状态就是枚举所有可能的树结构，因此过程如下：

a、首先枚举所有可能的树结构，即  q；

b、计算每种树结构下的目标函数值，即等式7的值；

c、取目标函数最小（大）值为最佳的数结构，根据等式6求得每个叶子节点的 $w$ 取值，即样本的预测值。

但上面的方法肯定是不可行的，因为树的结构千千万，所以一般用贪心策略来优化：

a、从深度为0的树开始，对每个叶节点枚举所有的可用特征

b、 针对每个特征，把属于该节点的训练样本根据该特征值升序排列，通过线性扫描的方式来决定该特征的最佳分裂点，并记录该特征的最大收益（采用最佳分裂点时的收益）

c、 选择收益最大的特征作为分裂特征，用该特征的最佳分裂点作为分裂位置，把该节点生长出左右两个新的叶节点，并为每个新节点关联对应的样本集

d、回到第1步，递归执行到满足特定条件为止

那么如何计算上面的收益呢，很简单，仍然紧扣目标函数就可以了。假设我们在某一节点上二分裂成两个节点，分别是左（L）右（R），则分列前的目标函数是:$-\frac{1}{2}\left[\frac{\left(G_{L}+G_{R}\right)^{2}}{H_{L}+H_{R}+\lambda}\right]+\gamma$,分裂后$-\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}\right]+2 \gamma$，则对于目标函数来说，分裂后的收益是（这里假设是最小化目标函数，所以用分裂前-分裂后）
Gain $=\frac{1}{2}\left[\frac{G_{L}^{2}}{H_{L}+\lambda}+\frac{G_{R}^{2}}{H_{R}+\lambda}-\frac{\left(G_{L}+G_{R}\right)^{2}}{H_{L}+H_{R}+\lambda}\right]-\gamma$

## 总结
a、算法在拟合的每一步都新生成一颗决策树；

b、在拟合这棵树之前，需要计算损失函数在每个样本上的一阶导和二阶导，即 $g_i$ 和 $h_i$ ；

c、通过上面的贪心策略生成一颗树，计算每个叶子结点的的 $G_j$和 $H_j$ ，利用等式6计算预测值 $w$ ；

d、把新生成的决策树 $f_{t}(x)$ 加入 $\hat{y}_{i}^{t}=\hat{y}_{i}^{t-1}+\epsilon f_{t}\left(x_{i}\right)$ ，其中$\epsilon$ 为学习率，主要为了抑制模型的过拟合。

[【参考知乎机器学习-一文理解GBDT的原理-20171001】](https://zhuanlan.zhihu.com/p/29765582)
这篇文章的推导思路很清晰，建议多看几遍，虽然很多但是没有废话，慢一点可以理解的。
